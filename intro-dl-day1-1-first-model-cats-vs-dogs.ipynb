{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction to Deep Learning Training\nIn this notebook you will learn how to make in a few lines of code a model that distinguises cats from dogs in pictures, using the [fastai](https://docs.fast.ai/) library.\n\n## Disclaimer\nThis is widely based on the [fastbook](https://github.com/fastai/fastbook) created by [Jeremy Howard](https://github.com/jph00) and [Sylvain Grugger](https://github.com/sgugger). Lot's of code samples and text snippets are taken from the fastbook as it is a great ressource and I just took some bits left and right and adapted them slightly to give you a first intro, but I really recommend if you are interested to follow it yourself in more details.\n\n## Install\nIn kaggle the fastai library is already pre installed so you don't need to install it\n\nAlso make sure that you select, before running any cell, the GPU backend by going to the ‘Settings’ tab and in 'Environment’ and select ‘GPU P100’. As you will see you only get 30 hours of GPU per week. However, this should be more than enough for this training. If you want to be sure you can also switch back to a CPU once you have finished the model training part, however make sure that you saved your model before as when you switch to CPU it will restart the kernel and all you variables and executions will be lost (except if you in persistence chose to persit the variables).\n","metadata":{"id":"eSjydcRcBWP7","papermill":{"duration":0.024951,"end_time":"2022-04-27T15:57:24.995193","exception":false,"start_time":"2022-04-27T15:57:24.970242","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"In the cell below you will find a complete system for creating and training a state-of-the-art model for recognizing cats versus dogs. So, let's train it now! To do so, just press Shift-Enter on your keyboard, or press the Play button on the toolbar. Then wait a few minutes while the following things happen:\n\n1. A dataset called the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) that contains 7,349 images of cats and dogs from 37 different breeds will be downloaded from the fast.ai datasets collection to the GPU server you are using, and will then be extracted.\n2. A *pretrained model* that has already been trained on 1.3 million images, using a competition-winning model will be downloaded from the internet.\n3. The pretrained model will be *fine-tuned* using the latest advances in transfer learning, to create a model that is specially customized for recognizing dogs and cats. It will run two epochs, one only on \n\nThe first two steps only need to be run once on your GPU server. If you run the cell again, it will use the dataset and model that have already been downloaded, rather than downloading them again. Let's take a look at the contents of the cell, and the results","metadata":{"id":"qcGJPzGHEWmD","papermill":{"duration":0.023342,"end_time":"2022-04-27T15:57:25.043093","exception":false,"start_time":"2022-04-27T15:57:25.019751","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'  # downloads the data\n\ndef is_cat(x): return x[0].isupper()  # label function (if uppercase it is a cat, otherwise it is a dog)\n\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))  # data loader\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)  # our CNN model\nlearn.fine_tune(1)  # training of the model","metadata":{"id":"ZwtVH7LHEDLw","outputId":"dfdaa266-9f85-42eb-90e3-05607a941ece","papermill":{"duration":184.87946,"end_time":"2022-04-27T16:00:29.946053","exception":false,"start_time":"2022-04-27T15:57:25.066593","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:28:40.262535Z","iopub.execute_input":"2023-03-15T08:28:40.263211Z","iopub.status.idle":"2023-03-15T08:31:39.725470Z","shell.execute_reply.started":"2023-03-15T08:28:40.263172Z","shell.execute_reply":"2023-03-15T08:31:39.724280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's have a look at what happened in a bit more details and how we can make sure our model actually works that well. The first line will download and decompress the dataset. It will only do this download once, and return the location of the decompressed archive. We can check what is inside with the .ls() method.\n","metadata":{"id":"dMolu-pxLW5x","papermill":{"duration":0.027765,"end_time":"2022-04-27T16:00:30.002121","exception":false,"start_time":"2022-04-27T16:00:29.974356","status":"completed"},"tags":[]}},{"cell_type":"code","source":"path = untar_data(URLs.PETS)/'images'  # won't redownload the data\npath.ls()  # show where it is saved and some paths","metadata":{"id":"V4qFh9UXLWi6","outputId":"a3316338-d1ad-47f9-80f5-80ac7eb0d038","papermill":{"duration":0.054417,"end_time":"2022-04-27T16:00:30.084191","exception":false,"start_time":"2022-04-27T16:00:30.029774","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:31:39.728477Z","iopub.execute_input":"2023-03-15T08:31:39.728896Z","iopub.status.idle":"2023-03-15T08:31:39.757201Z","shell.execute_reply.started":"2023-03-15T08:31:39.728855Z","shell.execute_reply":"2023-03-15T08:31:39.756306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"get_image_files is a fastai function that helps us grab all the image files (recursively) in one folder.","metadata":{"id":"m1qPG6DHL7vo","papermill":{"duration":0.027504,"end_time":"2022-04-27T16:00:30.139799","exception":false,"start_time":"2022-04-27T16:00:30.112295","status":"completed"},"tags":[]}},{"cell_type":"code","source":"files = get_image_files(path)\nlen(files)  # how many images do we have","metadata":{"id":"SrJHvCRiEg_S","outputId":"0beae5f4-7571-480e-dd4b-ab5d7fde58c2","papermill":{"duration":0.231775,"end_time":"2022-04-27T16:00:30.399297","exception":false,"start_time":"2022-04-27T16:00:30.167522","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:31:39.758402Z","iopub.execute_input":"2023-03-15T08:31:39.758735Z","iopub.status.idle":"2023-03-15T08:31:39.815263Z","shell.execute_reply.started":"2023-03-15T08:31:39.758700Z","shell.execute_reply":"2023-03-15T08:31:39.814190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Cats vs dogs\n\nTo label our data for the cats vs dogs problem, we need to know which filenames are of dog pictures and which ones are of cat pictures. There is an easy way to distinguish: the name of the file begins with a capital for cats, and a lowercased letter for dogs:","metadata":{"id":"vWAmEo2PMHbU","papermill":{"duration":0.028227,"end_time":"2022-04-27T16:00:30.455972","exception":false,"start_time":"2022-04-27T16:00:30.427745","status":"completed"},"tags":[]}},{"cell_type":"code","source":"files[0],files[6]","metadata":{"id":"-HpdExfiMHCE","outputId":"675df40c-929e-471b-daa2-7cf935bdc7ed","papermill":{"duration":0.037046,"end_time":"2022-04-27T16:00:30.521224","exception":false,"start_time":"2022-04-27T16:00:30.484178","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:23.711136Z","iopub.execute_input":"2023-03-15T08:32:23.712195Z","iopub.status.idle":"2023-03-15T08:32:23.719276Z","shell.execute_reply.started":"2023-03-15T08:32:23.712155Z","shell.execute_reply":"2023-03-15T08:32:23.718145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is why we defined our label function as follows:\n","metadata":{"id":"2P4XqZKAMTa0","papermill":{"duration":0.028691,"end_time":"2022-04-27T16:00:30.579150","exception":false,"start_time":"2022-04-27T16:00:30.550459","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def is_cat_(x): return x[0].isupper()  # label function (if uppercase it is a cat, otherwise it is a dog)","metadata":{"id":"LJVKo95RMBeu","papermill":{"duration":0.035681,"end_time":"2022-04-27T16:00:30.645766","exception":false,"start_time":"2022-04-27T16:00:30.610085","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:25.501848Z","iopub.execute_input":"2023-03-15T08:32:25.502553Z","iopub.status.idle":"2023-03-15T08:32:25.510327Z","shell.execute_reply.started":"2023-03-15T08:32:25.502514Z","shell.execute_reply":"2023-03-15T08:32:25.509355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get our data ready for a model, we need to put it in a `DataLoaders` object. Here we have a function that labels using the file names, so we will use `ImageDataLoaders.from_name_func`. There are other factory methods of `ImageDataLoaders` that could be more suitable for your problem, so make sure to check them all in `vision.data`. ","metadata":{"id":"1nfLt2ITMn0X","papermill":{"duration":0.028842,"end_time":"2022-04-27T16:00:30.703702","exception":false,"start_time":"2022-04-27T16:00:30.674860","status":"completed"},"tags":[]}},{"cell_type":"code","source":"dls = ImageDataLoaders.from_name_func(path, files, valid_pct=0.2, seed=42,\n                                      label_func=is_cat, item_tfms=Resize(224))","metadata":{"id":"j1qvg2NKMdHR","papermill":{"duration":0.462811,"end_time":"2022-04-27T16:00:31.195615","exception":false,"start_time":"2022-04-27T16:00:30.732804","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:27.022712Z","iopub.execute_input":"2023-03-15T08:32:27.023459Z","iopub.status.idle":"2023-03-15T08:32:27.432169Z","shell.execute_reply.started":"2023-03-15T08:32:27.023420Z","shell.execute_reply":"2023-03-15T08:32:27.431151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are various different classes for different kinds of deep learning datasets and problems—here we're using `ImageDataLoaders`. The first part of the class name will generally be the type of data you have, such as image, or text.\n\nThe other important piece of information that we have to tell fastai is how to get the labels from the dataset. Computer vision datasets are normally structured in such a way that the label for an image is part of the filename, or path—most commonly the parent folder name. fastai comes with a number of standardized labeling methods, and ways to write your own. Here we're telling fastai to use the `is_cat` function we just defined.\n\nFinally, we define the `Transform`s that we need. A `Transform` contains code that is applied automatically during training; fastai includes many predefined `Transform`s, and adding new ones is as simple as creating a Python function. There are two kinds: `item_tfms` are applied to each item (in this case, each item is resized to a 224-pixel square), while `batch_tfms` are applied to a *batch* of items at a time using the GPU, so they're particularly fast (we'll see many examples of these throughout this course).\n\nWhy 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but you can pass pretty much anything. If you increase the size, you'll often get a model with better results (since it will be able to focus on more details), but at the price of speed and memory consumption; the opposite is true if you decrease the size. \n\nThe most important parameter to mention here is `valid_pct=0.2`. This tells fastai to hold out 20% of the data and *not use it for training the model at all*. This 20% of the data is called the *validation set*; the remaining 80% is called the *training set*. The validation set is used to measure the accuracy of the model. By default, the 20% that is held out is selected randomly. The parameter `seed=42` sets the *random seed* to the same value every time we run this code, which means we get the same validation set every time we run it—this way, if we change our model and retrain it, we know that any differences are due to the changes to the model, not due to having a different random validation set.\n\nfastai will *always* show you your model's accuracy using *only* the validation set, *never* the training set. This is absolutely critical, because if you train a large enough model for a long enough time, it will eventually memorize the label of every item in your dataset! The result will not actually be a useful model, because what we care about is how well our model works on *previously unseen images*. That is always our goal when creating a model: for it to be useful on data that the model only sees in the future, after it has been trained.\n\nEven when your model has not fully memorized all your data, earlier on in training it may have memorized certain parts of it. As a result, the longer you train for, the better your accuracy will get on the training set; the validation set accuracy will also improve for a while, but eventually it will start getting worse as the model starts to memorize the training set, rather than finding generalizable underlying patterns in the data. When this happens, we say that the model is *overfitting*.","metadata":{"id":"jYfmrzpENo5T","papermill":{"duration":0.028957,"end_time":"2022-04-27T16:00:31.254308","exception":false,"start_time":"2022-04-27T16:00:31.225351","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"We can then check if everything looks okay with the `show_batch` method (`True` is for cat, `False` is for dog):","metadata":{"id":"Q2piUkrqOGzS","papermill":{"duration":0.028709,"end_time":"2022-04-27T16:00:31.313195","exception":false,"start_time":"2022-04-27T16:00:31.284486","status":"completed"},"tags":[]}},{"cell_type":"code","source":"dls.show_batch()","metadata":{"id":"LVPFQgAaNoSn","outputId":"fbcc5fbd-211c-4805-9013-1d4083dde8c9","papermill":{"duration":1.091551,"end_time":"2022-04-27T16:00:32.433417","exception":false,"start_time":"2022-04-27T16:00:31.341866","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:28.642173Z","iopub.execute_input":"2023-03-15T08:32:28.642793Z","iopub.status.idle":"2023-03-15T08:32:30.121919Z","shell.execute_reply.started":"2023-03-15T08:32:28.642736Z","shell.execute_reply":"2023-03-15T08:32:30.121035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The fifth line of the code training our image recognizer tells fastai to create a *convolutional neural network* (CNN) and specifies what *architecture* to use (i.e. what kind of model to create), what data we want to train it on, and what *metric* to use:\n\n```python\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\n```\n\nWhy a CNN? It's the current state-of-the-art approach to creating computer vision models. \n\nThere are many different architectures in fastai. Most of the time, however, picking an architecture isn't a very important part of the deep learning process. It's something that academics love to talk about, but in practice it is unlikely to be something you need to spend much time on. There are some standard architectures that work most of the time, and in this case we're using one called _ResNet_ that we'll be talking a lot about during the course; it is both fast and accurate for many datasets and problems. The `34` in `resnet34` refers to the number of layers in this variant of the architecture (other options are `18`, `50`, `101`, and `152`). Models using architectures with more layers take longer to train, and are more prone to overfitting (i.e. you can't train them for as many epochs before the accuracy on the validation set starts getting worse). On the other hand, when using more data, they can be quite a bit more accurate.","metadata":{"id":"IaKpzpc6Oa9D","papermill":{"duration":0.040142,"end_time":"2022-04-27T16:00:32.513818","exception":false,"start_time":"2022-04-27T16:00:32.473676","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"`cnn_learner` also has a parameter `pretrained`, which defaults to `True` (so it's used in this case, even though we haven't specified it), which sets the weights in your model to values that have already been trained by experts to recognize a thousand different categories across 1.3 million photos (using the famous [*ImageNet* dataset](http://www.image-net.org/)). A model that has weights that have already been trained on some other dataset is called a *pretrained model*. You should nearly always use a pretrained model, because it means that your model, before you've even shown it any of your data, is already very capable. And, as you'll see, in a deep learning model many of these capabilities are things you'll need, almost regardless of the details of your project. For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks.\n\nWhen using a pretrained model, `cnn_learner` will remove the last layer, since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights, of an appropriate size for the dataset you are working with. This last part of the model is known as the *head*.\n\nUsing pretrained models is the *most* important method we have to allow us to train more accurate models, more quickly, with less data, and less time and money. Using a pretrained model for a task different to what it was originally trained for is known as *transfer learning*. Unfortunately, because transfer learning is so under-studied, few domains have pretrained models available. For instance, there are currently few pretrained models available in medicine, making transfer learning challenging to use in that domain. In addition, it is not yet well understood how to use transfer learning for tasks such as time series analysis.","metadata":{"id":"grptWAx2Oy4Y","papermill":{"duration":0.038869,"end_time":"2022-04-27T16:00:32.591992","exception":false,"start_time":"2022-04-27T16:00:32.553123","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The sixth line of our code tells fastai how to *fit* the model:\n\n```python\nlearn.fine_tune(1)\n```\n\nAs we've discussed, the architecture only describes a *template* for a mathematical function; it doesn't actually do anything until we provide values for the millions of parameters it contains.\n\nThis is the key to deep learning—determining how to fit the parameters of a model to get it to solve your problem. In order to fit a model, we have to provide at least one piece of information: how many times to look at each image (known as number of *epochs*). The number of epochs you select will largely depend on how much time you have available, and how long you find it takes in practice to fit your model. If you select a number that is too small, you can always train for more epochs later.\n\nBut why is the method called `fine_tune`, and not `fit`? fastai actually *does* have a method called `fit`, which does indeed fit a model (i.e. look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we've started with a pretrained model, and we don't want to throw away all those capabilities that it already has. As you'll learn, there are some important tricks to adapt a pretrained model for a new dataset—a process called *fine-tuning*.","metadata":{"id":"JvBNi6ykPTLF","papermill":{"duration":0.039341,"end_time":"2022-04-27T16:00:32.671468","exception":false,"start_time":"2022-04-27T16:00:32.632127","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"When you use the `fine_tune` method, fastai will use these tricks for you. There are a few parameters you can set (which we'll discuss later), but in the default form shown here, it does two steps:\n\n1. Use one epoch to fit just those parts of the model necessary to get the new random head to work correctly with your dataset.\n1. Use the number of epochs requested when calling the method to fit the entire model, updating the weights of the later layers (especially the head) faster than the earlier layers (which, as we'll see, generally don't require many changes from the pretrained weights).\n\nThe *head* of a model is the part that is newly added to be specific to the new dataset. An *epoch* is one complete pass through the dataset. After calling `fit`, the results after each epoch are printed, showing the epoch number, the training and validation set losses (the \"measure of performance\" used for training the model), and any *metrics* you've requested (error rate, in this case).","metadata":{"id":"DacCnLYPPisA","papermill":{"duration":0.040511,"end_time":"2022-04-27T16:00:32.752117","exception":false,"start_time":"2022-04-27T16:00:32.711606","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Saving our model\nOnce you've got a model you're happy with, you need to save it, so that you can then copy it over to a server where you'll use it in production. Remember that a model consists of two parts: the *architecture* and the trained *parameters*. The easiest way to save the model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters. To save both parts, use the `export` method.\n\nThis method even saves the definition of how to create your `DataLoaders`. This is important, because otherwise you would have to redefine how to transform your data in order to use your model in production. fastai automatically uses your validation set `DataLoader` for inference by default, so your data augmentation will not be applied, which is generally what you want.\n\nWhen you call `export`, fastai will save a file called \"export.pkl\":","metadata":{"papermill":{"duration":0.04025,"end_time":"2022-04-27T16:00:32.833618","exception":false,"start_time":"2022-04-27T16:00:32.793368","status":"completed"},"tags":[]}},{"cell_type":"code","source":"learn.export('/kaggle/working/export.pkl')","metadata":{"papermill":{"duration":0.216367,"end_time":"2022-04-27T16:00:33.090048","exception":false,"start_time":"2022-04-27T16:00:32.873681","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:33.865738Z","iopub.execute_input":"2023-03-15T08:32:33.866363Z","iopub.status.idle":"2023-03-15T08:32:34.041801Z","shell.execute_reply.started":"2023-03-15T08:32:33.866325Z","shell.execute_reply":"2023-03-15T08:32:34.040767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check that the file exists, by using the `ls` method that fastai adds to Python's `Path` class:","metadata":{"papermill":{"duration":0.039158,"end_time":"2022-04-27T16:00:33.169018","exception":false,"start_time":"2022-04-27T16:00:33.129860","status":"completed"},"tags":[]}},{"cell_type":"code","source":"path = Path('/kaggle/working')\npath.ls(file_exts='.pkl')","metadata":{"papermill":{"duration":0.050232,"end_time":"2022-04-27T16:00:33.258610","exception":false,"start_time":"2022-04-27T16:00:33.208378","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:35.978812Z","iopub.execute_input":"2023-03-15T08:32:35.979971Z","iopub.status.idle":"2023-03-15T08:32:35.987698Z","shell.execute_reply.started":"2023-03-15T08:32:35.979921Z","shell.execute_reply":"2023-03-15T08:32:35.986571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the Kaggle notebooks you will notice that there is a third option in the “Add Data” modal: Notebook Output Files.\n\nUp to 20 GBs of output from a Notebook may be saved to disk in /kaggle/working. This data is saved automatically and you can then reuse that data in any future Notebook: just navigate to the “Data” pane in a Notebook editor, click on “Add Data”, click on the \"Notebook Output Files\" tab, find a Notebook of interest, and then click to add it to your current Notebook.\n\nBy chaining Notebooks as data sources in this way, it’s possible to build pipelines and generate more and better content than you could in a single notebook alone.","metadata":{"papermill":{"duration":0.039172,"end_time":"2022-04-27T16:00:33.339596","exception":false,"start_time":"2022-04-27T16:00:33.300424","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"When we use a model for getting predictions, instead of training, we call it *inference*. To create our inference learner from the exported file, we use `load_learner` (in this case, this isn't really necessary, since we already have a working `Learner` in our notebook; we're just doing it here so you can see the whole process end-to-end):","metadata":{"papermill":{"duration":0.039285,"end_time":"2022-04-27T16:00:33.418027","exception":false,"start_time":"2022-04-27T16:00:33.378742","status":"completed"},"tags":[]}},{"cell_type":"code","source":"learn_inf = load_learner('/kaggle/working/export.pkl')","metadata":{"papermill":{"duration":0.109478,"end_time":"2022-04-27T16:00:33.566670","exception":false,"start_time":"2022-04-27T16:00:33.457192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:40.475739Z","iopub.execute_input":"2023-03-15T08:32:40.476708Z","iopub.status.idle":"2023-03-15T08:32:40.550379Z","shell.execute_reply.started":"2023-03-15T08:32:40.476670Z","shell.execute_reply":"2023-03-15T08:32:40.549308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing our model on new data\nSo, how do we know if this model is any good? In the last column of the table you can see the error rate, which is the proportion of images that were incorrectly identified. The error rate serves as our metric—our measure of model quality, chosen to be intuitive and comprehensible. As you can see, the model is nearly perfect, even though the training time was only a few minutes (not including the one-time downloading of the dataset and the pretrained model). In fact, the accuracy you've achieved already is far better than anybody had ever achieved just 10 years ago!\n\nFinally, let's check that this model actually works. Go and get a photo of a dog, or a cat; if you don't have one handy, just search Google Images and download an image that you find there. Now execute the cell with `uploader` defined. It will output a button you can click, so you can select the image you want to classify:","metadata":{"papermill":{"duration":0.039257,"end_time":"2022-04-27T16:00:33.646216","exception":false,"start_time":"2022-04-27T16:00:33.606959","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from ipywidgets import widgets","metadata":{"papermill":{"duration":0.047104,"end_time":"2022-04-27T16:00:33.732587","exception":false,"start_time":"2022-04-27T16:00:33.685483","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:43.767651Z","iopub.execute_input":"2023-03-15T08:32:43.768357Z","iopub.status.idle":"2023-03-15T08:32:43.773720Z","shell.execute_reply.started":"2023-03-15T08:32:43.768317Z","shell.execute_reply":"2023-03-15T08:32:43.772441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uploader = widgets.FileUpload()\nuploader","metadata":{"id":"00i73IoLNrNA","papermill":{"duration":0.054919,"end_time":"2022-04-27T16:00:33.826898","exception":false,"start_time":"2022-04-27T16:00:33.771979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:36:21.239026Z","iopub.execute_input":"2023-03-15T08:36:21.239496Z","iopub.status.idle":"2023-03-15T08:36:21.257855Z","shell.execute_reply.started":"2023-03-15T08:36:21.239453Z","shell.execute_reply":"2023-03-15T08:36:21.256840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For kaggle automatic run, we can't actually click an upload button, so we fake it\n!wget https://github.com/fastai/fastbook/blob/master/images/chapter1_cat_example.jpg?raw=true","metadata":{"papermill":{"duration":2.584903,"end_time":"2022-04-27T16:00:36.451742","exception":false,"start_time":"2022-04-27T16:00:33.866839","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:53.595394Z","iopub.execute_input":"2023-03-15T08:32:53.596117Z","iopub.status.idle":"2023-03-15T08:32:55.743409Z","shell.execute_reply.started":"2023-03-15T08:32:53.596076Z","shell.execute_reply":"2023-03-15T08:32:55.742122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For kaggle automatic run, we can't actually click an upload button, so we fake it\nuploader = SimpleNamespace(data = ['chapter1_cat_example.jpg?raw=true'])","metadata":{"papermill":{"duration":0.077149,"end_time":"2022-04-27T16:00:36.600048","exception":false,"start_time":"2022-04-27T16:00:36.522899","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:32:55.746147Z","iopub.execute_input":"2023-03-15T08:32:55.746980Z","iopub.status.idle":"2023-03-15T08:32:55.753632Z","shell.execute_reply.started":"2023-03-15T08:32:55.746933Z","shell.execute_reply":"2023-03-15T08:32:55.752517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = PILImage.create(uploader.data[0])\nimg.to_thumb(224)","metadata":{"papermill":{"duration":0.075786,"end_time":"2022-04-27T16:00:36.722699","exception":false,"start_time":"2022-04-27T16:00:36.646913","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:36:30.662606Z","iopub.execute_input":"2023-03-15T08:36:30.663104Z","iopub.status.idle":"2023-03-15T08:36:30.813196Z","shell.execute_reply.started":"2023-03-15T08:36:30.663058Z","shell.execute_reply":"2023-03-15T08:36:30.812065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred,_,probs = learn_inf.predict(uploader.data[0])\nprint(f\"Is this a cat?: {pred}.\")\nprint(f\"Probability it's a cat: {probs[1].item():.6f}\")","metadata":{"papermill":{"duration":0.250677,"end_time":"2022-04-27T16:00:37.018632","exception":false,"start_time":"2022-04-27T16:00:36.767955","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-15T08:36:34.901334Z","iopub.execute_input":"2023-03-15T08:36:34.901831Z","iopub.status.idle":"2023-03-15T08:36:35.357764Z","shell.execute_reply.started":"2023-03-15T08:36:34.901749Z","shell.execute_reply":"2023-03-15T08:36:35.356648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Excercises\n\n1. Try out the model with a picture of your own dog/cat or with one from the internet\n2. Try to build the same model without using any transfer learning (check the `pretrained` parameter of [cnn_learner](https://docs.fast.ai/vision.learner.html#vision_learner))","metadata":{"papermill":{"duration":0.046257,"end_time":"2022-04-27T16:00:37.111012","exception":false,"start_time":"2022-04-27T16:00:37.064755","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.055563,"end_time":"2022-04-27T16:00:37.214679","exception":false,"start_time":"2022-04-27T16:00:37.159116","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}